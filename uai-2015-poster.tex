%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% From a template maintained at https://github.com/jamesrobertlloyd/cbl-tikz-poster
%
% Code near the top should be fairly standard and not need to be changed
%  - except for the document class
% Code lower down is more likely to be customised
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[portrait,a0b,final,a4resizeable]{include/a0poster}


\usepackage{multicol}
\usepackage{color}
\usepackage{morefloats}
\usepackage[pdftex]{graphicx}
\usepackage{rotating}
\usepackage{amsmath, amsthm, amssymb, bm}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}


\usepackage{include/picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]
\definecolor{darkblue}{rgb}{0,0.08,0.45}
\definecolor{blue}{rgb}{0,0,1}



\usepackage{dsfont}
\input{include/jlposter.tex}



%\def \figPath{../uai2014-GPSABC/}
\def \figPath{images/}

\input{cdefs}
\begin{document}
\begin{poster}

% Potentially add some space at the top of the poster
\vspace{0\baselineskip}



%%% Header
\begin{center}
\begin{pcolumn}{0.99}

\newcommand{\logowidth}{0.099\textwidth}  % width mauna decomp
%\newcommand{\logowidth}{0.099\textwidth}  % width mauna decomp

\pbox{0.99\textwidth}{}{linewidth=2mm,framearc=0.3,linecolor=camdarkblue,fillstyle=gradient,gradangle=0,gradbegin=white,gradend=white,gradmidpoint=1.0,framesep=1em}{
%
%%% Cambridge Logo
\begin{minipage}[c]{\logowidth}
  \begin{center}
    \includegraphics[width=14cm,clip=true,trim=0.5cm 1.5cm 0.5cm 1.5cm]{badges/University-of-Amsterdam-logo.png}
  \end{center}
\end{minipage}
%
%%% Title
\begin{minipage}[c][9cm][c]{0.76\textwidth}
  \begin{center}
    {\sffamily \VeryHuge \textbf{Hamiltonian ABC}}\\[10mm]
    {\huge\sffamily \Huge Edward Meeds and Robert Leenders and  Max Welling\\[7.5mm]
    %\texttt{\{ti242, dkd23, zoubin\}@cam.ac.uk}
    }
  \end{center}
\end{minipage}
%
%
% Harvard logo
\begin{minipage}[c]{\logowidth}
  \begin{flushright}
    \includegraphics[width=8cm]{badges/500px-University_of_Amsterdam_logo.png}
  \end{flushright}
\end{minipage}

}
\end{pcolumn}
\end{center}

%\vspace*{1.5cm}

\large


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Beginning of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{multicols}{3}



\mysection{Introduction}

%\vspace{0.5in}

%\begin{minipage}[c]{0.17\columnwidth}
\begin{itemize}
	\item Scientists express hypotheses through complex and expensive simulators.
	\item Posterior distributions give insight into models for both understanding underlying phenomena and improving hypotheses.
	\item Approximate Bayesian Computation provides a Bayesian framework for posterior analysis, but is very inefficient.
  \item This work uses a surrogate of the simulator to speed-up ABC.
  \item Based on the \emph{Metropolis-Hastings Error}, our algorithms determine when a simulation is necessary and provides the user with a ``knob'' $\xi$ to control it.
\end{itemize}

%\vspace{0.5in}

\mysection{Approximate Bayesian Computation}
%\vspace{-2cm}
\begin{tabular}{cc}
%\begin{minipage}[c]{0.73\columnwidth}
\begin{minipage}[c]{0.57\columnwidth}
  % put description of blowfly here
  \begin{itemize}
    %\item Bayesian inference: $\p( \thetav | \y ) = \frac{\p(\thetav) \p( \y | \thetav)}{\int \p(\thetav) \p( \y | \thetav) d \thetav}$
  	\item ABC is a \emph{likelihood-free} method because $\p( \y | \thetav)$ is either not computable or very expensive: \item[] \begin{equation}
  	\p_\epsilon( \thetav | \y ) = \frac{\p(\thetav)}{\p(\y)} \int \pi_\epsilon(\y|\x) \p(\x|\thetav) d \x \nonumber
  \end{equation}
      %\end{itemize}
    
      \item Kernel functions $\pi_\epsilon(\y|\x)$ are proxies for the likelihood, based on draws $\x \overset{\simulator}{\sim} \pi( \x| \thetav )$ from simulator.
      \item Rejection sampling with $\epsilon$-tube kernel is very inefficient. 
      \item We use ABC MCMC, which approximates the likelihood by Monte Carlo approximation:
      \begin{equation}
       \p_\epsilon(\y|\thetapv) \approx \frac{1}{S} \sum_{s=1}^S \pi_{\epsilon}(\y | \x^{(s)}, \thetapv ) \nonumber
      \end{equation}
      We accept the proposed parameter $\thetapv$ with probability equal to $\alpha( \thetapv | \thetav )  = $
      \begin{equation}
      \min \lp 1,  \frac{\pi(\thetapv) \sum_s \pi_{\epsilon}(\y | \xps, \thetapv )q( \thetav | \thetapv) }{\pi(\thetav) \sum_s \pi_{\epsilon}(\y | \x^{(s)}, \thetav )q( \thetapv | \thetav) } \rp \nonumber
      \end{equation} 
  \end{itemize}
\end{minipage}
&
%\begin{minipage}[c]{0.39\columnwidth}
\begin{minipage}[c]{0.43\columnwidth}
  % put blowfly problem pic here
  \begin{centering}
  \begin{tabular}{c}
  \includegraphics[width=\columnwidth,clip=true,trim=2.0cm 1cm 1cm 2cm]{\figPath exponential_problem.eps} \\
  \begin{minipage}[c]{\columnwidth}
    \begin{tabular}{cc}
      \multicolumn{2}{c}{$\eps$ trades-off precision and mixing}  \\
      \includegraphics[width=0.45\columnwidth,clip=true,trim=1.5cm 1.0cm 1.5cm 1.0cm]{\figPath abc_mcmc_eps_0p5.png} &
      \includegraphics[width=0.45\columnwidth,clip=true,trim=1.5cm 1.0cm 1.5cm 1.0cm]{\figPath abc_mcmc_eps_0p01.png}\\
      $\eps=0.5$ & $\eps=0.01$
    \end{tabular}
  \end{minipage}\\
  
  %Convergence to observations: \\
  %\hspace{-0.0cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 1cm 0.61cm]{\figPath blowfly_conv_per_sims.pdf} \\
  \end{tabular}
  \end{centering}
\end{minipage} 

\end{tabular}


\mysection{The Synthetic Likelihood}
\begin{itemize}
  \item Introduced by Wood (2010), replace the Monte Carlo approximation with a Gaussian with estimators based on the  pseudo-data $\{\x_1,..,\x_S\}$ simulated at $\thetav$:
  \begin{align}
  	\muhattheta    &=  \frac{1}{S} \sum_s \x^{(s)} & 
  	\Sigmahattheta &=  \frac{1}{S-1} \sum_s \lp \x^{(s)} - \muhattheta \rp \lp \x^{(s)} - \muhattheta \rp^T \nonumber
  \end{align}
  \item With a Gaussian kernel:
  \begin{equation}
  \pi_{\eps}(\y|\x) = \Keps\lp \y, \x \rp = \frac{1}{(2\pi\eps)^{J/2}} e^{-\frac{1}{2\epsilon^2}\lp\x-\y\rp^T \lp\x-\y\rp }\nonumber
  \end{equation}
  \item The synthetic likelihood can then be computed analytically:
  \begin{align}
  	\pi(\y|\thetav) &= \int \Keps\lp \y, \x \rp \mathcal{N}\lp \muhattheta,\Sigmahattheta\rp d\x \nonumber 
  	= \mathcal{N}\lp \muhattheta,\Sigmahattheta + \epsilon^2\eye \rp \nonumber
  \end{align}
\end{itemize}


\newpage 
\mysection{MCMC with a Random Acceptance Probability}


\begin{tabular}{cc}
\begin{minipage}[c]{0.65\columnwidth}
  \begin{itemize}
    \item The randomness of $\muhattheta$ (by re-running $S$ simulations), induces a distribution over the acceptance probabilities.
    \item Approximate randomness in $\mutheta$ by drawing $M$ times $\muthetam \sim \mathcal{N}\lp \muhattheta,\Sigmahattheta/S\rp$
    \item Estimate $p(\alpha)$ using Monte Carlo approximation based on these $M$ samples: 
    % \begin{align}
    %   p(\muthetapm) &= \mathcal{N}\lp \muhattheta,\Sigmahattheta\rp \\
    %    \alpham  &=  \min \lp 1,  \frac{\pi(\thetapv)\mathcal{N}\lp \y | \muthetapm, \Sigmahatthetap + \epsilon^2\eye \rp q( \thetav | \thetapv) }{\pi(\thetav) \mathcal{N}\lp \y | \muthetam, \Sigmahattheta + \epsilon^2\eye \rp q( \thetapv | \thetav) } \rp
    % \end{align}
    \begin{equation}
    		\alpham  =  \min \lp 1,  \frac{\pi(\thetapv)\mathcal{N}\lp \y | \muthetapm, \Sigmahatthetap + \epsilon^2\eye \rp q( \thetav | \thetapv) }{\pi(\thetav) \mathcal{N}\lp \y | \muthetam, \Sigmahattheta + \epsilon^2\eye \rp q( \thetapv | \thetav) } \rp  \nonumber
    \end{equation}
  \end{itemize}
\end{minipage}
&
\begin{minipage}[c]{0.35\columnwidth}
\centering
\begin{tabular}{c}
  $p(\alpha )$ and $F(\alpha)$  \\ 
  \includegraphics[width=\columnwidth]{\figPath acceptance_distribution_with_fold.png}
\end{tabular}
\end{minipage}
\end{tabular}
\begin{itemize}
  \item Based on $p(\alpha)$, compute the probability of making a M-H Error.  This is the area under the folded CDF and is minimized at the median of $p(\alpha)$, denoted $\tau$. \\ \\
  \vspace{0.5cm}
  \begin{minipage}[c]{\columnwidth}
    \begin{tabular}{cc}
      {\bf Probability of acceptance error} & {\bf Probability of rejection error} \\
      $P( \alpha < u ) = \frac{1}{M} \sum_m \lb \alpham < u\rb$ & $P( \alpha > u ) = \frac{1}{M} \sum_m \lb \alpham \geq u\rb$  
    \end{tabular}
  \end{minipage}
   \vspace{0.5cm}
  \item Total {\em conditional} error: $\errorcond = \lb u \leq \tau\rb P( \alpha < u ) + \lb u > \tau\rb P( \alpha \geq u )$
  \item MHE: $\erroruncond = \int \errorcond \mathcal{U}(0,1) d u$ 
  \item The crux of the MH step is to run simulations while $\erroruncond > \xi$.
\end{itemize}

%\newcommand{\gpdrawbox}[1]{
%\setlength\fboxsep{0pt}
%\hspace{-0.36in} 
% \fbox{
% \includegraphics[width=0.23\columnwidth]{../figures/deep_draws/deep_gp_sample_layer_#1}
% }}

% As depth increases, there is usually only one direction we can move $\vx$ to change $\vy$.
%
% \vspace{0.5in}
% We also visualize a distribution warped by successive functions drawn from a \gpt{} prior:
% \vspace{0.5in}
%
% \centering
% \begin{tabular}{cccc}
% $p(\vx)$ & $p(\vf^{(1)}(\vx))$ & $p(\vf^{(1:4)}(\vx))$ &  $p(\vf^{(1:6)}(\vx))$ \\
% %\gpdrawbox{1} & \gpdrawbox{2} & \gpdrawbox{4} & \gpdrawbox{6} \\
% \end{tabular}
% As depth increases, the density concentrates along one-dimensional filaments.
% \vspace{0.3in}


\mysection{Gaussian Process Surrogate ABC}

\begin{itemize}
  \item Adaptive SL is wasteful for expensive simulations: all results are discarded.
  \item GPS-ABC follow directly from synthetic likelihood ABC with randomized acceptance.
  \item Gaussian processes provides uncertainty estimates of the marginal likelihood informing us of the need to conduct additional experiments in order to make confident accept/reject decisions.
  \item For each statistic $j$, the surrogate provides the following conditional predictive distribution of the expected value of statistic $j$ based on $N$ training points $\muthetaj \sim	\mathcal{N}\lp \muthetajbar, \sigmasqtheta \rp $
  \begin{equation}
    \muthetajbar 
  	=
    \kthetaN 
  	\lb
  	\KNN + \sigma^2_j \eye
  	\rb^{-1}
  	\X[:,j] \nonumber
  \end{equation}
  \begin{equation}
      \sigmasqtheta  =  
    \kthetatheta 
    -
    \kthetaN 
  	\lb
  	\KNN + \sigma^2_j \eye
  	\rb^{-1}
    \kthetaN \nonumber
  \end{equation}
	\item Adjusting $\xi$ affects precision and computation:
  % (yellow -- model uncertainty, red -- training points):
\end{itemize}
\begin{tabular}{ccc}
  \begin{minipage}[c]{0.3\columnwidth}
    \centering
    $\xi = 0.4$, $N=9$
    \\
    \includegraphics[clip, trim = 1cm 1cm 2cm 1cm, width=\columnwidth]{\figPath toy_gp_s0_5_e_0p40_T_10000_gp_zoom_n_9.png}
    \\
    {\small high model uncertainty.}
  \end{minipage}
  &
  \begin{minipage}[c]{0.3\columnwidth}
    \centering
    $\xi = 0.2$, $N=184$
    \\
    \includegraphics[clip, trim = 1cm 1cm 2cm 1cm, width=\columnwidth]{\figPath toy_gp_s0_20_e_0p20_T_10000_gp_zoom_n_184.png}
    \\
    {\small model uncertainty drops with $\xi$}
  \end{minipage}
  &
  \begin{minipage}[c]{0.3\columnwidth}
    \centering
    $\xi = 0.05$, $N=1297$
    \\
    \includegraphics[clip, trim = 1cm 1cm 2cm 1cm, width=\columnwidth]{\figPath toy_gp_s0_20_e_0p05_T_10000_gp_zoom_n_1297.png} 
    \\
    {\small high precision, many simulations}
  \end{minipage}
\end{tabular}
\begin{itemize}
  \item[]
  \item The key advantage of GPS-ABC is that with increasing frequency, we will not have to do any expensive simulations whatsoever during a MH step because the GP surrogate is sufficiently confident about the statistics' surface in that region of parameter space. 
\end{itemize}






\newpage
\mysection{Toy problem: Inferring parameters of an exponential distributions}
\begin{itemize}
	\item GPS-ABC learns the surface, eventually eliminating any new simulations:
\end{itemize}
\begin{tabular}{ccc}
  \begin{minipage}[c]{0.3\columnwidth}
    \centering
    \includegraphics[clip, trim = 0.75cm 0.5cm 1cm 0.9cm, width=\columnwidth]{\figPath exp_paper1berror_v_samples.pdf}
    \\
    Error per sample
  \end{minipage}
  &
  \begin{minipage}[c]{0.3\columnwidth}
    \centering
    \includegraphics[clip, trim = 0.75cm 0.5cm 1cm 0.9cm, width=\columnwidth]{\figPath exp_paper1berror_v_both.pdf}
    \\
    Error per simulation
  \end{minipage}
  &
  \begin{minipage}[c]{0.3\columnwidth}
    \centering
    \includegraphics[clip, trim = 0.75cm 0.5cm 1cm 0.9cm, width=\columnwidth]{\figPath exp_paper1btime_v_sims.pdf} 
    \\
    Simulations per sample
  \end{minipage}
\end{tabular}
%\newcommand{\spectrumpic}[1]{
%\hspace{-0.2in}
%\includegraphics[trim=5mm 0mm 4mm 6.4mm, clip, width=0.25\columnwidth]{../figures/spectrum/layer-#1}} 


% \begin{tabular}{cc}
% \begin{minipage}[c]{0.73\columnwidth}
%
% %{\bf Theorem}
% %The Jacobian of a deep \gp{} with a product kernel is a product of independent Gaussian matrices, with each entry in each matrix being drawn independently.
%
% \begin{itemize}
% \item Jacobian of a deep GP is a product of independent Gaussian matrices.
% \item Singular values spectrum of Jacobian quantifies relative size of derivatives.
% \item As the net gets deeper, distribution of SVs becomes heavy-tailed, and the largest singular value dominates.
% %\item Eventually, there is only one direction we can move $\vx$, in order to change $\vy$.
% \end{itemize}
%
% %Normalized singular value spectrum of the Jacobian of a deep GP.    This implies that with high probability, there is only one effective degree of freedom in the representation being computed.  As depth increases, the distribution on singular values also becomes heavy-tailed.
%
% \end{minipage}
% &
% \begin{minipage}[c]{\columnwidth}
% \begin{centering}
% \begin{tabular}{ccc}
% 2 layer SV spectrum \\
% %\hspace{-0.16in} \spectrumpic{2} \\
% %4 layers &
% 6 layers SV spectrum \\
% %\hspace{-0.16in} \spectrumpic{6}
% %2 layers & 4 layers & 6 layers
% \end{tabular}
% \end{centering}
% \end{minipage}
% \end{tabular}



\mysection{Chaotic Ecological Systems}

\begin{tabular}{cc}
\begin{minipage}[c]{0.47\columnwidth}
%\begin{minipage}[c]{0.95\columnwidth}
  % put description of blowfly here
  \begin{itemize}
    \item Adult blowfly populations exhibit dynamic, sometimes chaotic, behavior for which several competing population models exist.  
    \item We use observational data and a simulation model from Wood (2010).
    %, based on their improvement upon previous population dynamics theory.
    \item Population dynamics are modeled using (discretized) differential equations that can produce chaotic behavior for some parameter settings.
    \item The population dynamics equation generates  $N_1, \ldots, N_T$:
    \begin{tabular}{rcl}
    	$N_{t+1}$    & $=$ & $P N_{t-\tau} \exp(-N_{t-\tau}/N_0) e_t$ \\
    	 &  & $ + N_t \exp(-\delta \epsilon_t)$
    \end{tabular}
    \item Injected noise: $e_t \sim  \mathcal{G}( 1/{\sigma_p^2},1/{\sigma_p^2})$,  $\epsilon_t  \sim  \mathcal{G}(1/{\sigma_d^2},1/{\sigma_d^2})$ 
    \item $\theta$: $\{ \log P, \log \delta, \log N_0, \log \sigma_d, \log \sigma_p, \tau\}$
    \item Statistics $\y$ ($J=10$): the log of the mean of all $25\%$ quantiles of $N/1000$, the mean of the $25\%$ quantiles of the first-order differences of $N/1000$, and the maximal peaks of smoothed $N$, with 2 different thresholds.
    \item Models: rejection sampling ($\eps=0.5$), SL ($S=10$), GPS-ABC ($\xi=0.3$) 
    \item Results: generated observations (top), posterior samples $\thetav$ (middle), convergence to $\ystar$ using posterior predictive $p(\y | \ystar )$ (bottom).
  \end{itemize}
\end{minipage}
&
%\begin{minipage}[c]{0.39\columnwidth}
\begin{minipage}[c]{0.47\columnwidth}
  % put blowfly problem pic here
  \begin{centering}
  \begin{tabular}{c}
  \hspace{-0.0cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 0cm 0.1cm]{\figPath blowfly_problem.pdf} \\
  %Convergence to observations: \\
  \hspace{-0.0cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 1cm 0.1cm]{\figPath blowfly_theta_given_y.pdf} \\
  \hspace{-0.0cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 1cm 0.1cm]{\figPath blowfly_conv_per_sims.pdf} \\
  \end{tabular}
  \end{centering}
\end{minipage} %\\
%
% \begin{minipage}[c]{0.47\columnwidth}
%   % put blowfly problem pic here
%   \begin{centering}
%   \begin{tabular}{c}
%   \hspace{-0.0cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 1cm 0.1cm]{\figPath blowfly_theta_given_y.pdf}
%   \end{tabular}
%   \end{centering}
% \end{minipage}
% &
% \begin{minipage}[c]{0.47\columnwidth}
%   % put blowfly problem pic here
%   \begin{centering}
%   \begin{tabular}{c}
%   \hspace{-0.0cm}\includegraphics[width=\columnwidth, clip, trim = 0cm 0cm 1cm 0.1cm]{\figPath blowfly_theta_given_y.pdf}
%   \end{tabular}
%   \end{centering}
%\end{minipage}

\end{tabular}








\mysection{Conclusions}
\begin{itemize}
  \item Adaptive ABC algorithms using Metropolis-Hastings Error controls the computational complexity of the inference
	\item GP surrogate models incorporate both model and pseudo-data uncertainty into MCMC
	\item Improvements: Hamiltonian dynamics on the GP surface for independent samples; covariance on the outputs; alternative surrogate models; acquisition functions.
\end{itemize}


% \raggedright
%
% \begin{tabular}{cc}
% \begin{minipage}[c]{0.8\columnwidth}
%
% Code at \texttt{github.com/duvenaud/deep-limits}
%
% Paper at \texttt{mlg.eng.cam.ac.uk/duvenaud/papers/verydeep.pdf}
%
% \end{minipage}
% &
% \begin{minipage}[c]{0.2\columnwidth}
% \begin{centering}
% %\includegraphics[width=\linewidth]{figures/qrcode-paper-link}
% \end{centering}
% \end{minipage}
% \end{tabular}
%
%
%
\end{multicols}
\end{poster}

\end{document}
